{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aff67d18-5e19-49da-8ba9-8cf324cb93b5",
   "metadata": {},
   "source": [
    "# Hand Gesture Classification\n",
    "### by Adrian Abraham"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c7e197-3af0-4273-980f-41172fc47139",
   "metadata": {},
   "source": [
    "## 1 - Package installation\n",
    "- **tensorflow**: for model creation and predictions\n",
    "- **pandas**: for csv_reading\n",
    "- **numpy**: for data storage and manipulation\n",
    "- **mediapipe**: contains pre-made hand detection module for data collection\n",
    "- **open-cv**: for live feed\n",
    "- **ast**: to convert string literal lists into actual lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "780c0847-b46b-4264-8ca2-f1ce2c6757d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (2.16.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from tensorflow) (2.32.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from tensorflow) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from tensorflow) (1.64.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.6.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7849341-8a20-464f-8ced-3baed76eb02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66b2ec0a-c9be-40d7-9ec1-09ec02481fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (0.10.14)\n",
      "Requirement already satisfied: absl-py in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from mediapipe) (23.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from mediapipe) (24.3.25)\n",
      "Requirement already satisfied: jax in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from mediapipe) (0.4.30)\n",
      "Requirement already satisfied: jaxlib in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from mediapipe) (0.4.30)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from mediapipe) (3.9.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: opencv-contrib-python in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from mediapipe) (4.10.0.84)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from mediapipe) (4.25.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from mediapipe) (0.4.7)\n",
      "Requirement already satisfied: CFFI>=1.0 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from jax->mediapipe) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from jax->mediapipe) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.9 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from jax->mediapipe) (1.14.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from matplotlib->mediapipe) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from matplotlib->mediapipe) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from matplotlib->mediapipe) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from matplotlib->mediapipe) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from matplotlib->mediapipe) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from matplotlib->mediapipe) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2a840ac-7d28-4951-9174-1e2a17933898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /opt/anaconda3/envs/tfsessions/lib/python3.10/site-packages (from opencv-python) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f79579-7689-45d4-841c-a5b9445e8aa0",
   "metadata": {},
   "source": [
    "## 2 - Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cfc2533-57c5-499a-91fc-aa6449e07334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input\n",
    "from tensorflow.keras.activations import linear, relu, sigmoid\n",
    "from tensorflow.keras.losses import BinaryCrossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce42d59-a0cc-42c5-a790-78e504668274",
   "metadata": {},
   "source": [
    "### Quick check that tensorflow is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39f709a9-b91c-4998-b783-c99096c1df6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.2\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934624a0-919a-452c-93a6-32c292eda3f7",
   "metadata": {},
   "source": [
    "## 3 - Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7691a255-9573-4012-8172-523a662d7e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive and negative data are stored in csv format.\n",
    "# We can use read_csv() to get the contents of each file\n",
    "df1 = pd.read_csv('peace.csv')\n",
    "df0 = pd.read_csv('not_peace.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18beb672-25d5-4188-9e8f-a7b59c1cf414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of                                                  coords\n",
       "0     [(0.6682692307692307, 1.0), (0.384615384615384...\n",
       "1     [(0.6634615384615384, 1.0), (0.379807692307692...\n",
       "2     [(0.6650717703349283, 1.0), (0.382775119617224...\n",
       "3     [(0.6523809523809524, 1.0), (0.376190476190476...\n",
       "4     [(0.6555023923444976, 1.0), (0.377990430622009...\n",
       "...                                                 ...\n",
       "1533  [(0.7243243243243244, 1.0), (0.491891891891891...\n",
       "1534  [(0.7297297297297297, 1.0), (0.497297297297297...\n",
       "1535  [(0.7297297297297297, 1.0), (0.491891891891891...\n",
       "1536  [(0.7336956521739131, 1.0), (0.5, 0.9354838709...\n",
       "1537  [(0.7336956521739131, 1.0), (0.5, 0.9354838709...\n",
       "\n",
       "[1538 rows x 1 columns]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9beb09a7-28c7-43e3-9e3f-4381d93ccb42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of                                                  coords\n",
       "0     [(0.75, 1.0), (0.5340909090909091, 0.946547884...\n",
       "1     [(0.7154696132596685, 1.0), (0.511049723756906...\n",
       "2     [(0.6941489361702128, 1.0), (0.486702127659574...\n",
       "3     [(0.6851851851851852, 1.0), (0.473544973544973...\n",
       "4     [(0.6487179487179487, 1.0), (0.446153846153846...\n",
       "...                                                 ...\n",
       "2007  [(0.7950530035335689, 1.0), (0.505300353356890...\n",
       "2008  [(0.7758620689655172, 1.0), (0.496551724137931...\n",
       "2009  [(0.75, 1.0), (0.48333333333333334, 0.93627450...\n",
       "2010  [(0.7435897435897436, 1.0), (0.493589743589743...\n",
       "2011  [(0.7272727272727273, 1.0), (0.482758620689655...\n",
       "\n",
       "[2012 rows x 1 columns]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c39a25c-88fc-4453-bac6-b26ba7cd332d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The format of our csv has a list of data as a string, \n",
    "# to turn them into lists so we use the ast library\n",
    "df1['coords'] = df1['coords'].apply(ast.literal_eval)\n",
    "positive_data = np.array(df1['coords'].tolist())\n",
    "\n",
    "df0['coords'] = df0['coords'].apply(ast.literal_eval)\n",
    "negative_data = np.array(df0['coords'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2298ee9-aa18-4405-ae99-04fccc72466d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.66826923, 1.        ],\n",
       "        [0.38461538, 0.93271462],\n",
       "        [0.24038462, 0.78190255],\n",
       "        ...,\n",
       "        [0.97115385, 0.59860789],\n",
       "        [0.85096154, 0.67981439],\n",
       "        [0.77403846, 0.75638051]],\n",
       "\n",
       "       [[0.66346154, 1.        ],\n",
       "        [0.37980769, 0.93518519],\n",
       "        [0.24038462, 0.78703704],\n",
       "        ...,\n",
       "        [0.97115385, 0.59953704],\n",
       "        [0.85096154, 0.68287037],\n",
       "        [0.77403846, 0.75925926]],\n",
       "\n",
       "       [[0.66507177, 1.        ],\n",
       "        [0.38277512, 0.93317972],\n",
       "        [0.23923445, 0.78571429],\n",
       "        ...,\n",
       "        [0.98086124, 0.60368664],\n",
       "        [0.86124402, 0.68663594],\n",
       "        [0.77511962, 0.76267281]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.72972973, 1.        ],\n",
       "        [0.49189189, 0.93261456],\n",
       "        [0.37837838, 0.78436658],\n",
       "        ...,\n",
       "        [0.95135135, 0.66846361],\n",
       "        [0.83243243, 0.74663073],\n",
       "        [0.76756757, 0.80862534]],\n",
       "\n",
       "       [[0.73369565, 1.        ],\n",
       "        [0.5       , 0.93548387],\n",
       "        [0.38043478, 0.78763441],\n",
       "        ...,\n",
       "        [0.95652174, 0.66666667],\n",
       "        [0.83695652, 0.74731183],\n",
       "        [0.77173913, 0.80913978]],\n",
       "\n",
       "       [[0.73369565, 1.        ],\n",
       "        [0.5       , 0.93548387],\n",
       "        [0.38043478, 0.79301075],\n",
       "        ...,\n",
       "        [0.95652174, 0.67204301],\n",
       "        [0.83152174, 0.75      ],\n",
       "        [0.76086957, 0.80913978]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6311ee7-c680-4c4e-8044-2b064d742e6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.75      , 1.        ],\n",
       "        [0.53409091, 0.94654788],\n",
       "        [0.36079545, 0.82628062],\n",
       "        ...,\n",
       "        [0.95738636, 0.47216036],\n",
       "        [0.98295455, 0.38975501],\n",
       "        [1.        , 0.30957684]],\n",
       "\n",
       "       [[0.71546961, 1.        ],\n",
       "        [0.51104972, 0.94130435],\n",
       "        [0.34530387, 0.8173913 ],\n",
       "        ...,\n",
       "        [0.95303867, 0.4826087 ],\n",
       "        [0.98066298, 0.4       ],\n",
       "        [1.        , 0.3173913 ]],\n",
       "\n",
       "       [[0.69414894, 1.        ],\n",
       "        [0.48670213, 0.93418259],\n",
       "        [0.32446809, 0.80679406],\n",
       "        ...,\n",
       "        [0.94414894, 0.48619958],\n",
       "        [0.97606383, 0.40127389],\n",
       "        [1.        , 0.32059448]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.75      , 1.        ],\n",
       "        [0.48333333, 0.93627451],\n",
       "        [0.31333333, 0.77941176],\n",
       "        ...,\n",
       "        [0.98      , 0.46078431],\n",
       "        [0.99333333, 0.37990196],\n",
       "        [1.        , 0.29901961]],\n",
       "\n",
       "       [[0.74358974, 1.        ],\n",
       "        [0.49358974, 0.93364929],\n",
       "        [0.32051282, 0.78199052],\n",
       "        ...,\n",
       "        [0.96794872, 0.45971564],\n",
       "        [0.98717949, 0.37677725],\n",
       "        [1.        , 0.29620853]],\n",
       "\n",
       "       [[0.72727273, 1.        ],\n",
       "        [0.48275862, 0.92986425],\n",
       "        [0.31661442, 0.78280543],\n",
       "        ...,\n",
       "        [0.96865204, 0.47285068],\n",
       "        [0.99059561, 0.38914027],\n",
       "        [1.        , 0.30995475]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a8c4bc3-552f-48b9-a9d2-a277959f1335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3550, 21, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can concatenate them together to have all our data in one np array\n",
    "data = np.concatenate((positive_data,negative_data))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6f414d4-66b9-40db-979a-dca81aa4b5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each training example can be either 1 or 0\n",
    "targets = np.zeros(data.shape[0])\n",
    "\n",
    "# We are setting each training example to have either 0 or 1 output\n",
    "for i in range(positive_data.shape[0]):\n",
    "    targets[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9377702-873d-42c3-b75f-77795937740b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,075</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m42\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)             │         \u001b[38;5;34m1,075\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m260\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m11\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,346</span> (5.26 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,346\u001b[0m (5.26 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,346</span> (5.26 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,346\u001b[0m (5.26 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Input(shape=(21,2)),          # Define the input shape here\n",
    "    Flatten(),                     # Flatten the (21, 2) input into a 1D vector\n",
    "    Dense(25, activation='relu'), # Fully connected layer with 128 neurons\n",
    "    Dense(10, activation='relu'),  # Fully connected layer with 64 neurons\n",
    "    Dense(1, activation='sigmoid') # Output layer for binary classification\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5518cd4-e576-44e8-8a47-ef51df630fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss=BinaryCrossentropy(),  # Use appropriate loss function for classification\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbd611f0-3d78-4980-94e5-fb6f8fcbf81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 530us/step - accuracy: 0.7025 - loss: 0.6444\n",
      "Epoch 2/40\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 474us/step - accuracy: 0.9842 - loss: 0.3205\n",
      "Epoch 3/40\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9945 - loss: 0.0916\n",
      "Epoch 4/40\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555us/step - accuracy: 1.0000 - loss: 0.0373\n",
      "Epoch 5/40\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 474us/step - accuracy: 1.0000 - loss: 0.0188\n",
      "Epoch 6/40\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 471us/step - accuracy: 1.0000 - loss: 0.0120\n",
      "Epoch 7/40\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 479us/step - accuracy: 1.0000 - loss: 0.0076\n",
      "Epoch 8/40\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 472us/step - accuracy: 1.0000 - loss: 0.0055\n",
      "Epoch 9/40\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 474us/step - accuracy: 1.0000 - loss: 0.0042\n",
      "Epoch 10/40\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 478us/step - accuracy: 1.0000 - loss: 0.0035\n",
      "Epoch 11/40\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 459us/step - accuracy: 1.0000 - loss: 0.0027\n",
      "Epoch 12/40\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 452us/step - accuracy: 1.0000 - loss: 0.0022\n",
      "Epoch 13/40\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 454us/step - accuracy: 1.0000 - loss: 0.0019\n",
      "Epoch 14/40\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 457us/step - accuracy: 1.0000 - loss: 0.0016\n",
      "Epoch 15/40\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 458us/step - accuracy: 1.0000 - loss: 0.0012   \n",
      "Epoch 16/40\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 544us/step - accuracy: 1.0000 - loss: 0.0011\n",
      "Epoch 17/40\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529us/step - accuracy: 1.0000 - loss: 9.6128e-04\n",
      "Epoch 18/40\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 473us/step - accuracy: 1.0000 - loss: 7.3984e-04\n",
      "Epoch 19/40\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 457us/step - accuracy: 1.0000 - loss: 6.9342e-04\n",
      "Epoch 20/40\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 459us/step - accuracy: 1.0000 - loss: 5.9733e-04\n",
      "Epoch 21/40\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 474us/step - accuracy: 1.0000 - loss: 5.9716e-04\n",
      "Epoch 22/40\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 470us/step - accuracy: 1.0000 - loss: 4.7867e-04\n",
      "Epoch 23/40\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 641us/step - accuracy: 1.0000 - loss: 4.1557e-04\n",
      "Epoch 24/40\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 473us/step - accuracy: 1.0000 - loss: 4.4003e-04\n",
      "Epoch 25/40\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 472us/step - accuracy: 1.0000 - loss: 3.6507e-04\n",
      "Epoch 26/40\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 472us/step - accuracy: 1.0000 - loss: 3.2912e-04\n",
      "Epoch 27/40\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 457us/step - accuracy: 1.0000 - loss: 2.8000e-04\n",
      "Epoch 28/40\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 475us/step - accuracy: 1.0000 - loss: 2.2452e-04\n",
      "Epoch 29/40\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 473us/step - accuracy: 1.0000 - loss: 2.4096e-04\n",
      "Epoch 30/40\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 480us/step - accuracy: 1.0000 - loss: 2.1688e-04\n",
      "Epoch 31/40\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 472us/step - accuracy: 1.0000 - loss: 1.8546e-04\n",
      "Epoch 32/40\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 476us/step - accuracy: 1.0000 - loss: 1.5922e-04\n",
      "Epoch 33/40\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 485us/step - accuracy: 1.0000 - loss: 1.7554e-04\n",
      "Epoch 34/40\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 473us/step - accuracy: 1.0000 - loss: 1.3074e-04\n",
      "Epoch 35/40\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 475us/step - accuracy: 1.0000 - loss: 1.2354e-04\n",
      "Epoch 36/40\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - accuracy: 1.0000 - loss: 1.1879e-04\n",
      "Epoch 37/40\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 474us/step - accuracy: 1.0000 - loss: 9.9868e-05\n",
      "Epoch 38/40\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 474us/step - accuracy: 1.0000 - loss: 1.0866e-04\n",
      "Epoch 39/40\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 476us/step - accuracy: 1.0000 - loss: 1.0951e-04\n",
      "Epoch 40/40\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 475us/step - accuracy: 1.0000 - loss: 8.2898e-05\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    data,targets,\n",
    "    epochs=40\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20e5f881-819a-45d6-ad31-cdb29c5e1d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "855c3e70-7c2c-4028-9820-dba64f000ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1720224939.244129 15070763 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M1\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1720224939.268491 15073106 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1720224939.274407 15073106 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False,\n",
    "                       max_num_hands=1,\n",
    "                       min_detection_confidence=0.5,\n",
    "                       min_tracking_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "399a2e64-8771-4aa1-8f88-3c3056c7c475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07/05 17:15:40.255893][info][15070763][Context.cpp:69] Context created with config: default config!\n",
      "[07/05 17:15:40.255976][info][15070763][Context.cpp:74] Context work_dir=/Users/adrianpaul1/Desktop/jupyter/hand-gesture-recognition/pythonProject1\n",
      "[07/05 17:15:40.255980][info][15070763][Context.cpp:77] \t- SDK version: 1.9.4\n",
      "[07/05 17:15:40.255982][info][15070763][Context.cpp:78] \t- SDK stage version: main\n",
      "[07/05 17:15:40.255988][info][15070763][Context.cpp:82] get config EnumerateNetDevice:false\n",
      "[07/05 17:15:40.255992][info][15070763][MacPal.cpp:36] createObPal: create MacPal!\n",
      "[07/05 17:15:40.257760][info][15070763][MacPal.cpp:104] Create PollingDeviceWatcher!\n",
      "[07/05 17:15:40.257792][info][15070763][DeviceManager.cpp:15] Current found device(s): (0)\n",
      "[07/05 17:15:40.257798][info][15070763][Pipeline.cpp:15] Try to create pipeline with default device.\n",
      "[07/05 17:15:40.257835][warning][15070763][ObException.cpp:5] No device found, fail to create pipeline!\n",
      "[07/05 17:15:40.260273][info][15070763][Context.cpp:90] Context destroyed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: not authorized to capture video (status 0), requesting...\n",
      "OpenCV: camera failed to properly initialize!\n",
      "[ WARN:0@3.347] global cap.cpp:323 open VIDEOIO(OBSENSOR): raised unknown C++ exception!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "BOX_MARGIN = 50\n",
    "while cap.isOpened():\n",
    "    # capturing the current frame\n",
    "    ret, frame = cap.read()\n",
    "    # getting window dimensions, shape contains height, width, and channels\n",
    "    height, width, _ = frame.shape\n",
    "\n",
    "    # if no frame returned, break\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    frame = cv2.flip(frame, 1)\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(frame_rgb)\n",
    "    if results.multi_hand_landmarks:\n",
    "        # for each hand found we calculate the coords of the landmarks\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            hand_coords = np.array([ (int(landmark.x * width), int(landmark.y * height)) for landmark in hand_landmarks.landmark ])\n",
    "        mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "        \n",
    "        hand_x_min, hand_x_max = min(x for x,y in hand_coords), max(x for x,y in hand_coords)\n",
    "        hand_y_min, hand_y_max = min(y for x,y in hand_coords), max(y for x,y in hand_coords)\n",
    "        \n",
    "        rect_start, rect_end = (hand_x_min - BOX_MARGIN, hand_y_min - BOX_MARGIN), (hand_x_max + BOX_MARGIN, hand_y_max + BOX_MARGIN)\n",
    "        rel_coords = np.array([ (x - rect_start[0], y - rect_start[1]) for x,y in hand_coords ])\n",
    "        \n",
    "        rel_x_max = max(x for x,y in rel_coords)\n",
    "        rel_y_max = max(y for x,y in rel_coords)\n",
    "\n",
    "        normalized_rel_coords = np.array([ (x / rel_x_max, y / rel_y_max) for x,y in rel_coords ])\n",
    "        normalized_rel_coords = np.expand_dims(normalized_rel_coords, axis=0)\n",
    "\n",
    "        prediction = model.predict(normalized_rel_coords)[0][0]\n",
    "        if prediction > 0.9:\n",
    "            text = f\"peace be with you, Confidence: {round(prediction * 100,2)}%\"\n",
    "        else:\n",
    "            text = f\"give me some peace bro, Confidence: {round(prediction * 100,2)}%\"\n",
    "\n",
    "        cv2.putText(frame, text, (50,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "        \n",
    "    cv2.imshow('Hand Landmarks', frame)\n",
    "\n",
    "    # when q pressed, we end video capture\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a253b8-326b-438b-ae97-51e34967a94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ea1e48-0213-4672-bf68-82b2082ca6d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
